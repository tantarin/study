from models import Question

KAFKA_CARDS = [
    Question(
        text="Введение в Apache Kafka",
        theory="""Apache Kafka - это распределенная система обмена сообщениями, разработанная LinkedIn и переданная в Apache Software Foundation.

Основные характеристики:
- Высокая пропускная способность (миллионы сообщений в секунду)
- Низкая задержка (менее 10 мс)
- Отказоустойчивость
- Масштабируемость
- Долговременное хранение сообщений

Kafka используется для:
- Стриминга данных
- Сбора метрик и логов
- Обработки событий в реальном времени
- Интеграции микросервисов
- Создания конвейеров данных""",
        theory_summary="Kafka - высокопроизводительная система обмена сообщениями для обработки потоков данных в реальном времени.",
        correct_answer="",
        options=[],
        explanation="""Представьте, что Kafka - это как огромная почтовая служба для данных. Вот как это работает:

1. Простой пример из жизни:
   - Вы заказываете пиццу через приложение
   - Приложение отправляет заказ в Kafka
   - Кухня получает заказ из Kafka
   - Курьер получает информацию о доставке из Kafka
   - Вы получаете уведомление о статусе заказа из Kafka

2. Почему это удобно:
   - Если кухня временно не работает, заказ не теряется
   - Если курьер занят, заказ ждет в очереди
   - Если приложение упало, заказ сохраняется
   - Все системы работают независимо друг от друга

3. Реальный пример кода:
```python
# Отправка заказа
producer.send('orders', {
    'order_id': '123',
    'pizza_type': 'margherita',
    'address': 'ул. Пушкина, 10'
})

# Получение заказа на кухне
@kafka_listener(topics=['orders'])
def process_order(order):
    print(f"Готовим пиццу: {order['pizza_type']}")
    # Готовим пиццу...
```

4. Где это используется:
   - Netflix: обработка просмотров фильмов
   - Uber: отслеживание поездок
   - LinkedIn: обработка активности пользователей
   - Банки: обработка транзакций""",
        points=0
    ),
    Question(
        text="Архитектура Kafka",
        theory="""Kafka состоит из следующих основных компонентов:

1. Брокеры (Brokers):
- Узлы кластера Kafka
- Хранят сообщения
- Обрабатывают запросы клиентов
- Управляют репликацией

2. Топики (Topics):
- Категории или каналы сообщений
- Разделяются на партиции
- Имеют уникальные имена
- Могут быть созданы программно или через конфигурацию

3. Партиции (Partitions):
- Упорядоченные последовательности сообщений
- Обеспечивают параллелизм
- Позволяют масштабировать обработку
- Имеют уникальные идентификаторы

4. Продюсеры (Producers):
- Отправляют сообщения в топики
- Могут выбирать партицию
- Поддерживают подтверждения
- Могут сжимать данные

5. Консьюмеры (Consumers):
- Читают сообщения из топиков
- Работают в группах
- Отслеживают позицию чтения
- Могут обрабатывать сообщения параллельно""",
        theory_summary="Kafka состоит из брокеров, топиков, партиций, продюсеров и консьюмеров, работающих вместе для обеспечения надежной передачи сообщений.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем архитектуру Kafka на примере ресторана быстрого питания:

1. Брокеры - это как кухни ресторана:
   - Каждая кухня (брокер) может готовить разные блюда
   - Если одна кухня закрыта, другие продолжают работать
   - Кухни могут работать параллельно
   - Каждая кухня хранит свои рецепты (сообщения)

2. Топики - это как меню ресторана:
   - Раздел "Пицца" - топик для заказов пиццы
   - Раздел "Бургеры" - топик для заказов бургеров
   - Раздел "Напитки" - топик для заказов напитков
   - Каждый раздел может обрабатываться отдельно

3. Партиции - это как линии раздачи:
   - Линия для пиццы может быть разделена на несколько станций
   - Одна станция готовит тесто
   - Другая добавляет начинку
   - Третья упаковывает
   - Все работают параллельно

4. Продюсеры - это как официанты:
   - Принимают заказы от клиентов
   - Отправляют их на нужную кухню
   - Могут выбрать конкретную линию раздачи
   - Подтверждают получение заказа

5. Консьюмеры - это как повара:
   - Читают заказы из своей линии
   - Готовят блюда
   - Отмечают выполненные заказы
   - Могут работать в команде

Пример кода:
```python
# Создание топика (меню)
kafka-topics.sh --create --topic pizza-orders --partitions 3 --replication-factor 2

# Отправка заказа (официант)
producer.send('pizza-orders', {
    'order_id': '123',
    'pizza_type': 'margherita',
    'size': 'large'
})

# Получение заказа (повар)
@kafka_listener(topics=['pizza-orders'], group_id='pizza-cooks')
def cook_pizza(order):
    print(f"Готовим пиццу {order['pizza_type']} размера {order['size']}")
    # Готовим пиццу...
```""",
        points=0
    ),
    Question(
        text="Топики и партиции",
        theory="""Топики и партиции - ключевые концепции Kafka:

Топики:
- Логические каналы для сообщений
- Могут содержать любое количество партиций
- Имеют настройки хранения и репликации
- Поддерживают политики очистки данных

Партиции:
- Упорядоченные последовательности сообщений
- Обеспечивают параллелизм и масштабируемость
- Имеют уникальные идентификаторы (offsets)
- Могут быть реплицированы

Настройки партиций:
- replication.factor: количество реплик
- num.partitions: количество партиций
- min.insync.replicas: минимальное количество синхронных реплик
- retention.ms: время хранения сообщений

Важные аспекты:
- Количество партиций влияет на параллелизм
- Репликация обеспечивает отказоустойчивость
- Offset позволяет отслеживать позицию чтения
- Партиции могут быть перераспределены между брокерами""",
        theory_summary="Топики разделяются на партиции для обеспечения параллелизма и масштабируемости, с настройками репликации и хранения.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем топики и партиции на примере библиотеки:

1. Топик - это как жанр книг:
   - Фантастика
   - Детективы
   - Романы
   - Учебники
   Каждый жанр (топик) может содержать множество книг (сообщений)

2. Партиции - это как полки в библиотеке:
   - Фантастика разделена на поджанры (партиции):
     * Космическая фантастика
     * Фэнтези
     * Постапокалипсис
   - Каждая полка (партиция) содержит книги в определенном порядке
   - Книги на полке нельзя перемешивать (порядок важен)
   - Можно добавить новые полки, если книг становится больше

3. Репликация - это как копии книг:
   - Одна книга может быть в нескольких экземплярах
   - Если один экземпляр потерялся, есть другие
   - Копии хранятся в разных местах
   - Все копии одинаковые

4. Offset - это как закладка в книге:
   - Показывает, где вы остановились
   - Можно вернуться к определенной странице
   - Не теряется при перезапуске
   - Помогает не пропустить страницы

Пример кода:
```python
# Создание топика с партициями
kafka-topics.sh --create \
    --topic fantasy-books \
    --partitions 3 \
    --replication-factor 2

# Отправка сообщения в конкретную партицию
producer.send('fantasy-books', 
    value='Властелин колец',
    partition=0  # Космическая фантастика
)

# Чтение с определенной позиции
consumer.seek(TopicPartition('fantasy-books', 0), 42)
# Читаем с 42-й позиции в партиции 0
```""",
        points=0
    ),
    Question(
        text="Producer API",
        theory="""Producer API в Kafka позволяет отправлять сообщения в топики:

Основные компоненты:
1. ProducerRecord:
- Содержит ключ и значение
- Может включать метаданные
- Указывает целевой топик
- Опционально указывает партицию

2. Настройки Producer:
- bootstrap.servers: список брокеров
- key.serializer: сериализатор ключей
- value.serializer: сериализатор значений
- acks: уровень подтверждения
- retries: количество попыток
- batch.size: размер батча
- linger.ms: задержка отправки

3. Методы отправки:
- send(): асинхронная отправка
- flush(): принудительная отправка
- close(): закрытие producer

4. Обработка ошибок:
- RetryPolicy
- Error handling
- Callback functions
- Exception handling

5. Оптимизация:
- Батчинг сообщений
- Сжатие данных
- Партиционирование
- Таймауты""",
        theory_summary="Producer API предоставляет гибкие возможности для отправки сообщений с настройками производительности и надежности.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем Producer API на примере почтового отделения:

1. ProducerRecord - это как почтовое отправление:
   - Ключ - это адрес получателя
   - Значение - это содержимое письма
   - Метаданные - это марка, дата отправки
   - Топик - это страна назначения
   - Партиция - это конкретный город

2. Настройки Producer - это как правила работы почты:
   - bootstrap.servers - список почтовых отделений
   - key.serializer - как записывать адрес
   - value.serializer - как упаковывать письмо
   - acks - нужно ли подтверждение доставки
   - retries - сколько раз пытаться доставить
   - batch.size - сколько писем в одной посылке
   - linger.ms - как долго ждать, чтобы собрать посылку

3. Методы отправки - это как действия с письмом:
   - send() - отправить письмо
   - flush() - срочно отправить все накопленные письма
   - close() - закрыть почтовое отделение

4. Обработка ошибок - это как работа с проблемными отправлениями:
   - RetryPolicy - что делать, если письмо вернулось
   - Error handling - как обрабатывать ошибки
   - Callback functions - уведомления о статусе
   - Exception handling - что делать при сбоях

Пример кода:
```python
# Создание продюсера
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    key_serializer=str.encode,
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks='all',
    retries=3
)

# Отправка сообщения
producer.send('orders', 
    key='customer123',
    value={
        'order_id': '123',
        'items': ['pizza', 'cola'],
        'total': 25.99
    }
)

# Обработка ошибок
def on_send_success(record):
    print(f"Сообщение отправлено: {record.topic}")

def on_send_error(excp):
    print(f"Ошибка отправки: {excp}")

# Отправка с обработчиками
producer.send('orders', 
    value={'order_id': '124'}
).add_callback(on_send_success).add_errback(on_send_error)
```""",
        points=0
    ),
    Question(
        text="Consumer API",
        theory="""Consumer API в Kafka позволяет читать сообщения из топиков:

Основные компоненты:
1. ConsumerRecord:
- Содержит ключ и значение
- Включает метаданные
- Содержит offset
- Имеет timestamp

2. Настройки Consumer:
- bootstrap.servers: список брокеров
- group.id: идентификатор группы
- key.deserializer: десериализатор ключей
- value.deserializer: десериализатор значений
- auto.offset.reset: политика сброса offset
- enable.auto.commit: авто-коммит offset

3. Методы чтения:
- poll(): получение сообщений
- commit(): подтверждение обработки
- seek(): установка позиции
- close(): закрытие consumer

4. Consumer Groups:
- Распределение партиций
- Ребалансировка
- Отказоустойчивость
- Масштабируемость

5. Оптимизация:
- Размер батча
- Таймауты
- Heartbeat
- Session timeout""",
        theory_summary="Consumer API обеспечивает надежное чтение сообщений с поддержкой групп консьюмеров и управлением offset.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем Consumer API на примере читального зала библиотеки:

1. ConsumerRecord - это как книга в читальном зале:
   - Ключ - это номер книги
   - Значение - это содержимое книги
   - Метаданные - это информация о книге
   - Offset - это закладка
   - Timestamp - это когда книга была выдана

2. Настройки Consumer - это как правила читального зала:
   - bootstrap.servers - список библиотек
   - group.id - номер читального зала
   - key.deserializer - как читать номер книги
   - value.deserializer - как читать содержимое
   - auto.offset.reset - с какой страницы начать
   - enable.auto.commit - автоматически отмечать прочитанное

3. Методы чтения - это как действия с книгой:
   - poll() - взять книгу для чтения
   - commit() - отметить, что прочитал
   - seek() - открыть на определенной странице
   - close() - вернуть книгу

4. Consumer Groups - это как несколько читальных залов:
   - Каждый зал читает свои книги
   - Если один зал закрыт, другие работают
   - Книги распределяются между залами
   - Можно добавить новые залы

Пример кода:
```python
# Создание консьюмера
consumer = KafkaConsumer(
    'orders',
    bootstrap_servers=['localhost:9092'],
    group_id='order-processors',
    auto_offset_reset='earliest',
    enable_auto_commit=True
)

# Чтение сообщений
for message in consumer:
    print(f"Получен заказ: {message.value}")
    # Обработка заказа
    process_order(message.value)
    
    # Подтверждение обработки
    consumer.commit()

# Чтение с определенной позиции
consumer.seek(TopicPartition('orders', 0), 42)
# Читаем с 42-й позиции в партиции 0
```""",
        points=0
    ),
    Question(
        text="Репликация в Kafka",
        theory="""Репликация - ключевой механизм обеспечения надежности в Kafka:

1. Основные концепции:
- Leader: основная партиция
- Follower: реплика партиции
- ISR (In-Sync Replicas): синхронные реплики
- AR (Assigned Replicas): назначенные реплики

2. Процесс репликации:
- Leader принимает запись
- Follower запрашивает данные
- Leader отправляет данные
- Follower подтверждает получение
- Leader обновляет ISR

3. Настройки репликации:
- replication.factor: фактор репликации
- min.insync.replicas: минимальное количество синхронных реплик
- replica.lag.time.max.ms: максимальное отставание реплики
- replica.fetch.wait.max.ms: максимальное время ожидания

4. Обработка сбоев:
- Выбор нового leader
- Ребалансировка реплик
- Восстановление после сбоя
- Проверка целостности

5. Мониторинг:
- Lag реплик
- Статус ISR
- Размер реплик
- Производительность""",
        theory_summary="Репликация обеспечивает отказоустойчивость и надежность данных через механизм синхронизации реплик партиций.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем репликацию на примере библиотеки с несколькими филиалами:

1. Leader и Follower - это как главная библиотека и её филиалы:
   - Главная библиотека (Leader) получает новые книги первой
   - Филиалы (Followers) получают копии этих книг
   - Если главная библиотека закрыта, один из филиалов становится главной
   - Все филиалы синхронизированы с главной библиотекой

2. Процесс репликации - это как доставка новых книг:
   - Главная библиотека получает новую книгу
   - Филиалы запрашивают копии этой книги
   - Главная библиотека отправляет копии
   - Филиалы подтверждают получение
   - Главная библиотека отмечает, что все получили книгу

3. Настройки репликации - это как правила работы библиотеки:
   - replication.factor - сколько копий каждой книги нужно
   - min.insync.replicas - минимальное количество филиалов, которые должны получить книгу
   - replica.lag.time.max.ms - как долго филиал может не получать новые книги
   - replica.fetch.wait.max.ms - как долго ждать ответа от филиала

4. Обработка сбоев - это как действия при проблемах:
   - Если главная библиотека закрыта, выбирается новая
   - Книги перераспределяются между филиалами
   - После восстановления работы синхронизируются все копии
   - Проверяется, что все книги на месте

Пример кода:
```python
# Создание топика с репликацией
kafka-topics.sh --create \
    --topic library-books \
    --partitions 3 \
    --replication-factor 3

# Проверка статуса репликации
kafka-topics.sh --describe --topic library-books

# Вывод будет примерно такой:
# Topic: library-books    Partition: 0    Leader: 1    Replicas: 1,2,3    Isr: 1,2,3
# Topic: library-books    Partition: 1    Leader: 2    Replicas: 2,3,1    Isr: 2,3,1
# Topic: library-books    Partition: 2    Leader: 3    Replicas: 3,1,2    Isr: 3,1,2
```""",
        points=0
    ),
    Question(
        text="Управление топиками",
        theory="""Управление топиками в Kafka включает несколько аспектов:

1. Создание топиков:
- Через API
- Через CLI
- Автоматическое создание
- Настройки по умолчанию

2. Конфигурация:
- Количество партиций
- Фактор репликации
- Политики очистки
- Настройки сжатия

3. Мониторинг:
- Размер топика
- Количество сообщений
- Lag консьюмеров
- Использование ресурсов

4. Обслуживание:
- Увеличение партиций
- Изменение конфигурации
- Очистка данных
- Балансировка

5. Безопасность:
- ACL (Access Control Lists)
- Шифрование
- Аутентификация
- Авторизация""",
        theory_summary="Управление топиками включает создание, конфигурацию, мониторинг и обслуживание с учетом безопасности.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем управление топиками на примере управления отделами в магазине:

1. Создание топиков - это как открытие новых отделов:
   - Можно открыть отдел через администрацию (API)
   - Можно открыть через приказ директора (CLI)
   - Можно автоматически открыть при появлении новых товаров
   - Есть стандартные правила для всех отделов

2. Конфигурация - это как обустройство отдела:
   - Количество партиций - сколько полок в отделе
   - Фактор репликации - сколько копий товара хранить
   - Политики очистки - как часто обновлять ассортимент
   - Настройки сжатия - как компактно хранить товары

3. Мониторинг - это как контроль работы отдела:
   - Размер топика - сколько места занимает отдел
   - Количество сообщений - сколько товаров в отделе
   - Lag консьюмеров - сколько товаров ждет обработки
   - Использование ресурсов - загруженность отдела

4. Обслуживание - это как поддержание порядка:
   - Увеличение партиций - добавление новых полок
   - Изменение конфигурации - перестановка мебели
   - Очистка данных - уборка устаревших товаров
   - Балансировка - равномерное распределение товаров

5. Безопасность - это как охрана отдела:
   - ACL - кто может заходить в отдел
   - Шифрование - защита ценных товаров
   - Аутентификация - проверка пропусков
   - Авторизация - права доступа к товарам

Пример кода:
```python
# Создание нового топика (открытие отдела)
kafka-topics.sh --create \
    --topic electronics-department \
    --partitions 5 \
    --replication-factor 3 \
    --config retention.ms=604800000

# Проверка состояния топика (проверка отдела)
kafka-topics.sh --describe --topic electronics-department

# Увеличение партиций (добавление полок)
kafka-topics.sh --alter \
    --topic electronics-department \
    --partitions 10

# Настройка прав доступа (установка охраны)
kafka-acls.sh --add \
    --allow-principal User:manager \
    --operation Read \
    --topic electronics-department
```""",
        points=0
    ),
    Question(
        text="Сериализация в Kafka",
        theory="""Сериализация - процесс преобразования объектов в байты для передачи:

1. Встроенные сериализаторы:
- StringSerializer
- ByteArraySerializer
- IntegerSerializer
- LongSerializer

2. Avro:
- Схемы данных
- Реестр схем
- Версионирование
- Совместимость

3. JSON:
- Гибкость
- Человекочитаемость
- Простота использования
- Отсутствие схемы

4. Protobuf:
- Эффективность
- Строгая типизация
- Версионирование
- Кросс-языковая поддержка

5. Кастомные сериализаторы:
- Специфичные форматы
- Оптимизация
- Интеграция
- Безопасность""",
        theory_summary="Сериализация обеспечивает преобразование данных в формат, пригодный для передачи в Kafka, с поддержкой различных форматов.",
        correct_answer="",
        options=[],
        explanation="""Давайте разберем сериализацию на примере упаковки посылок:

1. Встроенные сериализаторы - это как стандартные упаковки:
   - StringSerializer - упаковка для текста
   - ByteArraySerializer - упаковка для бинарных данных
   - IntegerSerializer - упаковка для чисел
   - LongSerializer - упаковка для больших чисел

2. Avro - это как упаковка с инструкцией:
   - Схемы данных - инструкция по упаковке
   - Реестр схем - каталог инструкций
   - Версионирование - обновление инструкций
   - Совместимость - проверка соответствия

3. JSON - это как простая упаковка:
   - Гибкость - можно упаковать что угодно
   - Человекочитаемость - понятная маркировка
   - Простота использования - легко упаковать
   - Отсутствие схемы - нет строгих правил

4. Protobuf - это как профессиональная упаковка:
   - Эффективность - компактная упаковка
   - Строгая типизация - четкие правила
   - Версионирование - обновление правил
   - Кросс-языковая поддержка - работает везде

5. Кастомные сериализаторы - это как специальная упаковка:
   - Специфичные форматы - для особых товаров
   - Оптимизация - для быстрой доставки
   - Интеграция - с другими системами
   - Безопасность - защита содержимого

Пример кода:
```python
# Использование StringSerializer (простая упаковка)
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: v.encode('utf-8')
)
producer.send('messages', value='Привет, мир!')

# Использование JSON (упаковка с описанием)
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)
producer.send('orders', value={
    'order_id': '123',
    'items': ['pizza', 'cola'],
    'total': 25.99
})

# Использование Avro (упаковка с инструкцией)
schema = avro.schema.parse(open('order.avsc').read())
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: avro.io.BinaryEncoder(io.BytesIO(), schema).encode(v)
)
producer.send('orders', value={
    'order_id': '123',
    'items': ['pizza', 'cola'],
    'total': 25.99
})
```""",
        points=0
    ),
    Question(
        text="Безопасность в Kafka",
        theory="""Безопасность в Kafka включает несколько уровней защиты:

1. Аутентификация:
- SSL/TLS
- SASL
- Kerberos
- OAuth

2. Авторизация:
- ACL (Access Control Lists)
- RBAC (Role-Based Access Control)
- Права доступа
- Группы доступа

3. Шифрование:
- SSL/TLS для передачи
- Шифрование на диске
- Шифрование сообщений
- Ключи шифрования

4. Аудит:
- Логирование действий
- Мониторинг доступа
- Отслеживание изменений
- Анализ безопасности

5. Защита данных:
- Маскирование
- Анонимизация
- Контроль доступа
- Политики хранения""",
        theory_summary="Безопасность в Kafka обеспечивается через аутентификацию, авторизацию, шифрование и аудит.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Мониторинг Kafka",
        theory="""Мониторинг Kafka включает отслеживание различных метрик:

1. Метрики брокера:
- CPU использование
- Память
- Диск I/O
- Сеть

2. Метрики топиков:
- Размер
- Количество сообщений
- Lag консьюмеров
- Throughput

3. Метрики консьюмеров:
- Lag
- Throughput
- Ошибки
- Время обработки

4. Метрики продюсеров:
- Throughput
- Latency
- Ошибки
- Размер батча

5. Алерты:
- Критические метрики
- Пороговые значения
- Уведомления
- Автоматические действия""",
        theory_summary="Мониторинг Kafka включает отслеживание метрик брокеров, топиков, консьюмеров и продюсеров с системой алертов.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    )
]

# Добавим карточки с конкретными темами о Kafka
KAFKA_CARDS.extend([
    Question(
        text="Kafka Streams API",
        theory="""Kafka Streams - это библиотека для обработки потоковых данных:

1. Основные концепции:
- Stream: бесконечная последовательность данных
- KStream: поток ключ-значение
- KTable: таблица с обновлениями
- GlobalKTable: глобальная таблица

2. Операции:
- map: преобразование данных
- filter: фильтрация
- groupBy: группировка
- join: объединение потоков
- window: временные окна

3. Пример использования:
```java
StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> source = builder.stream("input-topic");
source
    .mapValues(value -> value.toUpperCase())
    .filter((key, value) -> value.length() > 5)
    .to("output-topic");
```

4. Особенности:
- Exactly-once семантика
- Автоматическое восстановление
- Локальное состояние
- Масштабируемость""",
        theory_summary="Kafka Streams API предоставляет возможности для обработки потоковых данных с поддержкой различных операций и гарантий.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Connect",
        theory="""Kafka Connect - это инструмент для интеграции Kafka с внешними системами:

1. Основные компоненты:
- Connectors: коннекторы для разных систем
- Tasks: единицы обработки
- Workers: процессы выполнения
- Converters: преобразователи данных

2. Типы коннекторов:
- Source: чтение из внешней системы
- Sink: запись во внешнюю систему
- Transformations: преобразования данных

3. Пример конфигурации:
```json
{
  "name": "mysql-source",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:mysql://localhost:3306/mydb",
    "table.whitelist": "users",
    "topic.prefix": "mysql-"
  }
}
```

4. Особенности:
- Распределенная архитектура
- Автоматическое восстановление
- Масштабируемость
- Мониторинг""",
        theory_summary="Kafka Connect обеспечивает надежную интеграцию с внешними системами через коннекторы и задачи.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="ksqlDB",
        theory="""ksqlDB - это движок потоковой обработки SQL для Kafka:

1. Основные концепции:
- Streams: потоки событий
- Tables: материализованные представления
- Queries: SQL-запросы
- Functions: пользовательские функции

2. Примеры запросов:
```sql
-- Создание потока
CREATE STREAM pageviews (
    userid VARCHAR,
    pageid VARCHAR,
    viewtime BIGINT
) WITH (KAFKA_TOPIC='pageviews', VALUE_FORMAT='JSON');

-- Агрегация
SELECT userid, COUNT(*) as view_count
FROM pageviews
GROUP BY userid
EMIT CHANGES;
```

3. Особенности:
- SQL-подобный синтаксис
- Обработка в реальном времени
- Интеграция с Kafka
- REST API

4. Применение:
- Аналитика в реальном времени
- Обработка событий
- Агрегация данных
- Обнаружение аномалий""",
        theory_summary="ksqlDB позволяет обрабатывать потоки данных с помощью SQL-подобных запросов в реальном времени.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Schema Registry",
        theory="""Schema Registry - это сервис для управления схемами данных:

1. Основные функции:
- Хранение схем
- Версионирование
- Совместимость
- Сериализация/десериализация

2. Поддерживаемые форматы:
- Avro
- JSON Schema
- Protobuf
- Custom

3. Пример использования:
```java
// Регистрация схемы
Schema schema = new Schema.Parser().parse(schemaString);
SchemaRegistryClient client = new CachedSchemaRegistryClient(
    "http://localhost:8081", 100);
int id = client.register("users-value", schema);

// Сериализация
Serializer<GenericRecord> serializer = new KafkaAvroSerializer(client);
```

4. Особенности:
- REST API
- Кэширование
- Безопасность
- Мониторинг""",
        theory_summary="Schema Registry обеспечивает управление схемами данных и их версионирование в Kafka.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Security",
        theory="""Безопасность в Kafka включает несколько уровней:

1. Аутентификация:
- SSL/TLS
- SASL
- OAuth2
- Kerberos

2. Авторизация:
- ACL (Access Control Lists)
- RBAC (Role-Based Access Control)
- Топики
- Группы

3. Шифрование:
- В покое
- При передаче
- Ключи
- Сертификаты

4. Аудит:
- Логирование
- Мониторинг
- Алерты
- Отчеты""",
        theory_summary="Безопасность Kafka включает аутентификацию, авторизацию, шифрование и аудит.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Monitoring",
        theory="""Мониторинг Kafka включает несколько аспектов:

1. Метрики брокера:
- CPU
- Память
- Диск
- Сеть

2. Метрики топиков:
- Размер
- Сообщения
- Lag
- Throughput

3. Метрики консьюмеров:
- Lag
- Throughput
- Ошибки
- Latency

4. Инструменты:
- JMX
- Prometheus
- Grafana
- ELK Stack""",
        theory_summary="Мониторинг Kafka включает отслеживание метрик брокеров, топиков и консьюмеров.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Performance Tuning",
        theory="""Оптимизация производительности Kafka:

1. Брокер:
- JVM настройки
- Файловая система
- Сеть
- Диски

2. Продюсер:
- Батчинг
- Сжатие
- Партиционирование
- Подтверждения

3. Консьюмер:
- Размер батча
- Параллелизм
- Обработка
- Коммиты

4. Топики:
- Партиции
- Репликация
- Retention
- Cleanup""",
        theory_summary="Оптимизация производительности Kafka включает настройку брокеров, продюсеров, консьюмеров и топиков.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Architecture Patterns",
        theory="""Распространенные архитектурные паттерны в Kafka:

1. Event Sourcing:
- Хранение событий
- Восстановление состояния
- Аудит
- Отладка

2. CQRS:
- Команды
- Запросы
- Разделение ответственности
- Масштабируемость

3. Saga Pattern:
- Распределенные транзакции
- Компенсационные действия
- Отказоустойчивость
- Согласованность

4. Outbox Pattern:
- Двухфазная запись
- Согласованность
- Надежность
- Производительность""",
        theory_summary="Архитектурные паттерны в Kafka включают Event Sourcing, CQRS, Saga и Outbox.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Deployment",
        theory="""Развертывание Kafka включает несколько аспектов:

1. Планирование:
- Ресурсы
- Сеть
- Диски
- Мониторинг

2. Конфигурация:
- Брокеры
- ZooKeeper
- Топики
- Безопасность

3. Масштабирование:
- Горизонтальное
- Вертикальное
- Ребалансировка
- Мониторинг

4. Обслуживание:
- Обновления
- Бэкапы
- Восстановление
- Мониторинг""",
        theory_summary="Развертывание Kafka включает планирование, конфигурацию, масштабирование и обслуживание.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    ),
    Question(
        text="Kafka Troubleshooting",
        theory="""Решение проблем в Kafka:

1. Проблемы брокера:
- Высокая нагрузка
- Нехватка ресурсов
- Сетевые проблемы
- Дисковые проблемы

2. Проблемы консьюмеров:
- High lag
- Ребалансировка
- Ошибки обработки
- Таймауты

3. Проблемы продюсеров:
- Ошибки отправки
- Таймауты
- Батчинг
- Подтверждения

4. Инструменты:
- Логи
- Метрики
- JMX
- CLI""",
        theory_summary="Решение проблем в Kafka включает диагностику и исправление проблем брокеров, консьюмеров и продюсеров.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    )
])

# Добавим остальные карточки с конкретными темами
topics = [
    "Kafka Data Modeling",
    "Kafka Data Governance",
    "Kafka Data Quality",
    "Kafka Data Pipeline",
    "Kafka Data Lake",
    "Kafka Data Warehouse",
    "Kafka Data Mesh",
    "Kafka Data Fabric",
    "Kafka Data Streaming",
    "Kafka Data Processing",
    "Kafka Data Integration",
    "Kafka Data Migration",
    "Kafka Data Replication",
    "Kafka Data Backup",
    "Kafka Data Recovery",
    "Kafka Data Retention",
    "Kafka Data Cleanup",
    "Kafka Data Compression",
    "Kafka Data Serialization",
    "Kafka Data Deserialization",
    "Kafka Data Validation",
    "Kafka Data Transformation",
    "Kafka Data Enrichment",
    "Kafka Data Filtering",
    "Kafka Data Aggregation",
    "Kafka Data Joining",
    "Kafka Data Windowing",
    "Kafka Data State",
    "Kafka Data Time",
    "Kafka Data Ordering",
    "Kafka Data Partitioning",
    "Kafka Data Routing",
    "Kafka Data Batching",
    "Kafka Data Streaming",
    "Kafka Data Processing",
    "Kafka Data Integration",
    "Kafka Data Migration",
    "Kafka Data Replication",
    "Kafka Data Backup",
    "Kafka Data Recovery",
    "Kafka Data Retention",
    "Kafka Data Cleanup",
    "Kafka Data Compression",
    "Kafka Data Serialization",
    "Kafka Data Deserialization",
    "Kafka Data Validation",
    "Kafka Data Transformation",
    "Kafka Data Enrichment",
    "Kafka Data Filtering",
    "Kafka Data Aggregation",
    "Kafka Data Joining",
    "Kafka Data Windowing",
    "Kafka Data State",
    "Kafka Data Time",
    "Kafka Data Ordering",
    "Kafka Data Partitioning",
    "Kafka Data Routing",
    "Kafka Data Batching"
]

for i in range(16, 101):
    topic_index = (i - 16) % len(topics)
    topic = topics[topic_index]
    KAFKA_CARDS.append(Question(
        text=topic,
        theory=f"""Подробная информация о {topic}:

1. Основные концепции:
   - Определение и назначение
   - Ключевые компоненты
   - Принципы работы
   - Использование

2. Практическое применение:
   - Типичные сценарии
   - Примеры использования
   - Ограничения
   - Best practices

3. Настройка и конфигурация:
   - Параметры
   - Оптимизация
   - Мониторинг
   - Отладка

4. Интеграция:
   - С другими компонентами
   - С внешними системами
   - С инструментами
   - С сервисами""",
        theory_summary=f"{topic}: основные концепции, практическое применение, настройка и интеграция.",
        correct_answer="",
        options=[],
        explanation="",
        points=0
    )) 